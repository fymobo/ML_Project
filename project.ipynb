{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05a79df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953842aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "heart = pd.read_csv('/Users/yuqinhan1229/Desktop/heart_failure_clinical_records_dataset.csv')\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d35c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "## Distribution of continuous variables\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "## Find the continuous variable and set the color.\n",
    "figure = plt.figure(figsize=(30, 20))\n",
    "continues = ['age','creatinine_phosphokinase','ejection_fraction',\\\n",
    " 'platelets','serum_creatinine','serum_sodium','time']\n",
    "colors = ['red', 'orange', 'yellow', 'green', 'cyan', 'blue', 'purple']\n",
    "for i in range(1, 8):\n",
    "    plt.subplot(3,3,i)\n",
    "    sns.histplot(heart[continues[i-1]], color = colors[i-1], kde=True)\n",
    "    plt.xlabel(continues[i-1])\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21f4454",
   "metadata": {},
   "outputs": [],
   "source": [
    "## distribution transformation\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "t = np.array(list(heart['creatinine_phosphokinase'])).reshape(-1, 1)\n",
    "pt = PowerTransformer(method='box-cox', standardize=False)\n",
    "new = pt.fit_transform(t)\n",
    "heart['creatinine_phosphokinase'] = new\n",
    "\n",
    "t = np.array(list(heart['serum_creatinine'])).reshape(-1, 1)\n",
    "pt = PowerTransformer(method='box-cox', standardize=False)\n",
    "new = pt.fit_transform(t)\n",
    "heart['serum_creatinine'] = new\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41457e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(15, 10))\n",
    "modify = ['creatinine_phosphokinase','serum_creatinine']\n",
    "colors = ['orange', 'cyan']\n",
    "for i in range(1, 3):\n",
    "    plt.subplot(2,2,i)\n",
    "    sns.histplot(heart[continues[i-1]], color = colors[i-1], kde=True)\n",
    "    plt.xlabel(continues[i-1])\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf018f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling as pp\n",
    "report = pp.ProfileReport(heart)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb9963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi-variable analysis\n",
    "## Correlation analysis for continue variablies\n",
    "conti_heart = heart[continues]\n",
    "corrDf = conti_heart.corr()\n",
    "figure = plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(corrDf,annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe6e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysis between categorical variables and continues variables\n",
    "categoricals = ['anaemia','diabetes','high_blood_pressure','sex','smoking','DEATH_EVENT']\n",
    "cate_heart = heart[categoricals]\n",
    "cate_heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3574248",
   "metadata": {},
   "outputs": [],
   "source": [
    "## chi - test for smoking&others\n",
    "from sklearn.feature_selection import chi2\n",
    "dicts = dict()\n",
    "index = ('anaemia','diaetes', 'high_blood_pressure', 'sex')\n",
    "chi2_val, p_val = chi2(cate_heart.iloc[:, 0:4], cate_heart.iloc[:, 4])\n",
    "\n",
    "itr = 0\n",
    "for i in index:\n",
    "    string = f'smoking vs {i}'\n",
    "    dicts[string] = p_val[itr]\n",
    "    itr += 1\n",
    "for key, value in dicts.items():\n",
    "        print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d93a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['age','creatinine_phosphokinase','ejection_fraction',\\\n",
    " 'platelets','serum_creatinine','serum_sodium','time','DEATH_EVENT']\n",
    "ind_heart = heart[index]\n",
    "ind_heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2a8875",
   "metadata": {},
   "outputs": [],
   "source": [
    "## z - test for DEATH_EVENT & Others continues variablies\n",
    "import statsmodels.stats.weightstats as sw\n",
    "dicts = dict()\n",
    "index = ['age','creatinine_phosphokinase','ejection_fraction','platelets'\\\n",
    "         ,'serum_creatinine','serum_sodium','time']\n",
    "z_val, p_val = sw.ztest(ind_heart.iloc[:, 0:7], ind_heart.iloc[:, 7])\n",
    "\n",
    "itr = 0\n",
    "for i in index:\n",
    "    string = f'DEATH_EVENT vs {i}'\n",
    "    dicts[string] = p_val[itr]\n",
    "    itr += 1\n",
    "for key, value in dicts.items():\n",
    "        print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2640c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53113c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering\n",
    "heart = pd.read_csv('/Users/yuqinhan1229/Desktop/heart_failure_clinical_records_dataset.csv')\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA \n",
    "heart = heart.drop('DEATH_EVENT',axis=1) \n",
    "kmeans = KMeans(n_clusters=2,random_state=42).fit(heart.values)\n",
    "kmeans_2 = pd.DataFrame(kmeans.labels_,columns=['cluster'])\n",
    "\n",
    "pca = PCA(n_components = 2).fit(heart)\n",
    "pca_trans = pca.transform(heart)\n",
    "pca_trans_df = pd.DataFrame(pca_trans,columns=['pca1','pca2'])\n",
    "kmeans_2 = pd.concat([kmeans_2,pca_trans_df],axis=1)\n",
    "kmeans_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe94805",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.lmplot(x='pca1',y='pca2',data=kmeans_2,hue='cluster',fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9575f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_all = pd.read_csv('/Users/yuqinhan1229/Desktop/heart_failure_clinical_records_dataset.csv')\n",
    "kmeans_2 = pd.concat([kmeans_2,heart_all['DEATH_EVENT']],axis=1)\n",
    "fig = sns.lmplot(x='pca1',y='pca2',data=kmeans_2,\n",
    "                 fit_reg=False,row='cluster',col='DEATH_EVENT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766eb2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree\n",
    "## Spliting data set\n",
    "import torch\n",
    "train, test = train_test_split(heart,random_state =0, test_size = 0.3)\n",
    "X_train = train.iloc[:,0:12]\n",
    "Y_train = train.iloc[:,12]\n",
    "X_test = test.iloc[:,0:12]\n",
    "Y_test = test.iloc[:,12]\n",
    "trainset = torch.utils.data.TensorDataset(torch.from_numpy(X_train.values),\\\n",
    "                                        torch.from_numpy(Y_train.values))\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=2,\\\n",
    "                                          shuffle=True)\n",
    "testset = torch.utils.data.TensorDataset(torch.from_numpy(X_test.values),\\\n",
    "                                         torch.from_numpy(Y_test.values))\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=2,\\\n",
    "                                         shuffle=False,num_workers=2)\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(max_depth=5)\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "scores = clf.score(X_test, Y_test)\n",
    "print(\"Train set score：\" + str(clf.score(X_train, Y_train)))\n",
    "print(\"Test set score：\" + str(clf.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e019275",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart = pd.read_csv('/Users/yuqinhan1229/Desktop/heart_failure_clinical_records_dataset.csv')\n",
    "heart.drop(columns = ['sex', 'diabetes'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5aaf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simplified decision tree\n",
    "from sklearn import tree\n",
    "train, test = train_test_split(heart,random_state =0, test_size = 0.3)\n",
    "X_train = train.iloc[:,0:10]\n",
    "Y_train = train.iloc[:,10]\n",
    "X_test = test.iloc[:,0:10]\n",
    "Y_test = test.iloc[:,10]\n",
    "trainset = torch.utils.data.TensorDataset(torch.from_numpy(X_train.values),\\\n",
    "                                        torch.from_numpy(Y_train.values))\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=2,\\\n",
    "                                          shuffle=True)\n",
    "testset = torch.utils.data.TensorDataset(torch.from_numpy(X_test.values),\\\n",
    "                                         torch.from_numpy(Y_test.values))\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=2,\\\n",
    "                                         shuffle=False,num_workers=2)\n",
    "clf = tree.DecisionTreeClassifier(max_depth=5)\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "scores = clf.score(X_test, Y_test)\n",
    "print(\"Train set score：\" + str(clf.score(X_train, Y_train)))\n",
    "print(\"Test set score：\" + str(clf.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f566af76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve of decision tree\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "confusion_matrix(Y_test, y_pred)\n",
    "y_predprob = clf.predict_proba(X_test)\n",
    "metrics.roc_auc_score(Y_test,y_predprob[:,1])\n",
    "\n",
    "fpr, tpr, thersholds = roc_curve(Y_test, y_predprob[:,1], pos_label=1)\n",
    " \n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, 'k--', label='ROC (area = {0:.2f})'.format(roc_auc), lw=2)\n",
    " \n",
    "plt.xlim([-0.05, 1.05]) \n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')  \n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be175faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "print(\"Train set score：\" + str(lr.score(X_train, Y_train)))\n",
    "print(\"Test set score：\" + str(lr.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf5701",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "y_predprob = lr.predict_proba(X_test)\n",
    "metrics.roc_auc_score(Y_test,y_predprob[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885b1e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve of logistic regression\n",
    "y_pred = lr.predict(X_test)\n",
    "confusion_matrix(Y_test, y_pred)\n",
    "y_predprob = lr.predict_proba(X_test)\n",
    "metrics.roc_auc_score(Y_test,y_predprob[:,1])\n",
    "\n",
    "fpr, tpr, thersholds = roc_curve(Y_test, y_predprob[:,1], pos_label=1)\n",
    " \n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, 'k--', label='ROC (area = {0:.2f})'.format(roc_auc), lw=2)\n",
    " \n",
    "plt.xlim([-0.05, 1.05]) \n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')  \n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b06d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the fully connected neural network which contain ne hidden layer\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f25efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparameter\n",
    "input_size=12\n",
    "hidden_size=84\n",
    "num_classes=2\n",
    "num_epochs=10\n",
    "batch_size=5\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f30cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset and normalize\n",
    "import pandas as pd\n",
    "df=pd.read_csv('/Users/86132/heart_failure_clinical_records_dataset.csv')\n",
    "df_change=df.apply(lambda x:(x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "df_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df_change,random_state =1, test_size = 0.3)\n",
    "X_train = train.iloc[:,0:12]\n",
    "Y_train = train.iloc[:,12]\n",
    "X_test = test.iloc[:,0:12]\n",
    "Y_test = test.iloc[:,12]\n",
    "trainset = torch.utils.data.TensorDataset(torch.from_numpy(X_train.values),torch.from_numpy(Y_train.values))\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=5,shuffle=True)\n",
    "testset = torch.utils.data.TensorDataset(torch.from_numpy(X_test.values),torch.from_numpy(Y_test.values))\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=5,shuffle=False,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10137d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "import torch.nn.functional as F\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_classes):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        self.fc1=nn.Linear(input_size,hidden_size)\n",
    "        self.fc2=nn.Linear(hidden_size,num_classes)\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x=F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2500d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model=NeuralNet(input_size,hidden_size,num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45c307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a82e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "total_step=len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i,data in enumerate(train_loader):\n",
    "        inputs,labels=data\n",
    "        inputs=inputs.float()\n",
    "        labels=labels.long()\n",
    "        outputs=model(inputs)\n",
    "        loss=criterion(outputs,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(i+1)%5==0:\n",
    "            if(epoch+1)%5==0:\n",
    "                print('epoch[{}/{}],step[{}/{}],loss:{:.4f}'.format(epoch+1,num_epochs,i+1,total_step,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c296fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model and get the accuracy\n",
    "with torch.no_grad():\n",
    "    correct=0\n",
    "    total=0\n",
    "    for inputs,labels in test_loader:\n",
    "        inputs=inputs.reshape(-1,12).to(device).float()\n",
    "        labels=labels.to(device)\n",
    "        outputs=model(inputs)\n",
    "        _,predicted=torch.max(outputs.data,1)\n",
    "        total+=labels.size(0)\n",
    "        correct+=(predicted==labels).sum().item()\n",
    "    print('accuracy of the nework is {}%'.format(100*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aef19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the fully connected neural network which contain two hidden layers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb6b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameter\n",
    "input_size=12\n",
    "hidden_size_1=84\n",
    "hidden_size_2=84\n",
    "num_classes=2\n",
    "num_epoch=10\n",
    "batch_size=5\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset and normalize it\n",
    "import pandas as pd\n",
    "df=pd.read_csv('/Users/86132/heart_failure_clinical_records_dataset.csv')\n",
    "df_change1=df.apply(lambda x:(x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "df_change1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a7acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "train,test=train_test_split(df_change1,random_state=1,test_size=0.3)\n",
    "X_train=train.iloc[:,0:12]\n",
    "Y_train=train.iloc[:,12]\n",
    "X_test=test.iloc[:,0:12]\n",
    "Y_test=test.iloc[:,12]\n",
    "trainset = torch.utils.data.TensorDataset(torch.from_numpy(X_train.values),torch.from_numpy(Y_train.values))\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=5,shuffle=True)\n",
    "testset = torch.utils.data.TensorDataset(torch.from_numpy(X_test.values),torch.from_numpy(Y_test.values))\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=5,shuffle=False,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79882b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "class simpleNet(nn.Module):\n",
    "    def __init__(self,inputs,hidden_size_1,hidden_size_2,outputs):\n",
    "        super(simpleNet,self).__init__()\n",
    "        self.fc1=nn.Linear(inputs,hidden_size_1)\n",
    "        self.fc2=nn.Linear(hidden_size_1,hidden_size_1)\n",
    "        self.fc3=nn.Linear(hidden_size_2,outputs)\n",
    "    def forward(self,x):\n",
    "        x=self.fc1(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.fc2(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ddfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model=simpleNet(input_size,hidden_size_1,hidden_size_2,num_classes).to(device)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040bf189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "total_step=len(train_loader)\n",
    "for epoch in range(num_epoch):\n",
    "    for i,data in enumerate(train_loader):\n",
    "        inputs,labels=data\n",
    "        inputs=inputs.reshape(-1,12).to(device)\n",
    "        inputs=inputs.float()\n",
    "        labels=labels.long()\n",
    "        outputs=model(inputs)\n",
    "        loss=criterion(outputs,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1)%5==0:\n",
    "            if(epoch+1)%5==0:\n",
    "                print('epoch[{}/{}],step[{}/{}],loss:{:.4f}'.format(epoch+1,num_epoch,i+1,total_step,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe8dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model and get the accuracy\n",
    "with torch.no_grad():\n",
    "    correct=0\n",
    "    total=0\n",
    "    for inputs,labels in test_loader:\n",
    "        inputs=inputs.reshape(-1,12).to(device).float()\n",
    "        labels=labels.to(device)\n",
    "        outputs=model(inputs)\n",
    "        _,predicted=torch.max(outputs.data,1)\n",
    "        total+=labels.size(0)\n",
    "        correct+=(predicted==labels).sum().item()\n",
    "    print('accuracy of the network is {}%'.format(100*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the fully connected neural network which contain three hidden layers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec38e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameter\n",
    "input_size=12\n",
    "hidden_size_1=84\n",
    "hidden_size_2=84\n",
    "hidden_size_3=84\n",
    "num_classes=2\n",
    "num_epoch=10\n",
    "batch_size=5\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9102b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset and normalize it\n",
    "import pandas as pd\n",
    "df=pd.read_csv('/Users/86132/heart_failure_clinical_records_dataset.csv')\n",
    "df_change2=df.apply(lambda x:(x-np.min(x))/(np.max(x)-np.min(x)))\n",
    "df_change2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c129d5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "train,test=train_test_split(df_change2,random_state=2,test_size=0.3)\n",
    "X_train=train.iloc[:,0:12]\n",
    "Y_train=train.iloc[:,12]\n",
    "X_test=test.iloc[:,0:12]\n",
    "Y_test=test.iloc[:,12]\n",
    "trainset=torch.utils.data.TensorDataset(torch.from_numpy(X_train.values),torch.from_numpy(Y_train.values))\n",
    "train_loader=torch.utils.data.DataLoader(trainset,batch_size=5,shuffle=True)\n",
    "testset=torch.utils.data.TensorDataset(torch.from_numpy(X_test.values),torch.from_numpy(Y_test.values))\n",
    "test_loader=torch.utils.data.DataLoader(testset,batch_size=5,shuffle=False,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad546c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the network\n",
    "class tripleNet(nn.Module):\n",
    "    def __init__(self,inputs,hidden_size_1,hidden_size_2,hidden_size_3,outputs):\n",
    "        super(tripleNet,self).__init__()\n",
    "        self.fc1=nn.Linear(inputs,hidden_size_1)\n",
    "        self.fc2=nn.Linear(hidden_size_1,hidden_size_2)\n",
    "        self.fc3=nn.Linear(hidden_size_2,hidden_size_3)\n",
    "        self.fc4=nn.Linear(hidden_size_3,outputs)\n",
    "    def forward(self,x):\n",
    "        x=self.fc1(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.fc2(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.fc3(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb24569",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model=tripleNet(input_size,hidden_size_1,hidden_size_2,hidden_size_3,num_classes).to(device)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c70cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "total_step=len(train_loader)\n",
    "for epoch in range(num_epoch):\n",
    "    for i,data in enumerate(train_loader):\n",
    "        inputs,labels=data\n",
    "        inputs=inputs.reshape(-1,12).to(device)\n",
    "        inputs=inputs.float()\n",
    "        labels=labels.long()\n",
    "        outputs=model(inputs)\n",
    "        loss=criterion(outputs,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1)%5==0:\n",
    "            if (epoch+1)%5==0:\n",
    "                print('epoch[{}/{}],step[{}/{}],loss{:.4f}'.format(epoch+1,num_epoch,i+1,total_step,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c23cfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model and get the accuracy\n",
    "with torch.no_grad():\n",
    "    correct=0\n",
    "    total=0\n",
    "    for inputs,labels in test_loader:\n",
    "        inputs=inputs.reshape(-1,12).to(device).float()\n",
    "        labels=labels.to(device)\n",
    "        outputs=model(inputs)\n",
    "        _,predicted=torch.max(outputs.data,1)\n",
    "        total+=labels.size(0)\n",
    "        correct+=(predicted==labels).sum().item()\n",
    "    print('accuracy of the network is {}%'.format(100*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4970a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the ROC curve\n",
    "from sklearn.metrics import roc_curve,auc,roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf91d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp=MLPClassifier(random_state=1,max_iter=300)\n",
    "mlp=mlp.fit(X_train,Y_train)\n",
    "y_pred=mlp.predict(X_test)\n",
    "y_predprob=mlp.predict_proba(X_test)\n",
    "fpr,tpr,thersholds=roc_curve(Y_test,y_predprob[:,1],pos_label=1)\n",
    "roc_auc=auc(fpr,tpr)\n",
    "plt.plot(fpr,tpr,'k--',label='ROC (area = {0:.2f})'.format(roc_auc),lw=2)\n",
    "plt.xlim([-0.05,1.05])\n",
    "plt.ylim([-0.05,1.05])\n",
    "plt.xlabel('False Position Rate')\n",
    "plt.ylabel('True Position Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a6ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e83349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from patsy import dmatrices\n",
    "kf = KFold(n_splits = 5)\n",
    "y, X = dmatrices('DEATH_EVENT ~ age + anaemia + creatinine_phosphokinase + '\n",
    "                 'ejection_fraction + high_blood_pressure + '\n",
    "                 'platelets + serum_creatinine + serum_sodium + smoking + time', heart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dbab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation of decision tree\n",
    "scores = []\n",
    "for train, test in kf.split(X):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=5)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    scores.append(clf.score(X_test,y_test))\n",
    "print(scores)\n",
    "sums = 0\n",
    "for i in scores:\n",
    "    sums = i + sums\n",
    "D_sc = sums/5\n",
    "print('Scores of Decision tree is: ' + str(D_sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a4d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation of logistic regression\n",
    "scores = []\n",
    "for train, test in kf.split(X):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    scores.append(lr.score(X_test,y_test))\n",
    "print(scores)\n",
    "sums = 0\n",
    "for i in scores:\n",
    "    sums = i + sums\n",
    "L_sc = sums/5\n",
    "print('Scores of Decision tree is: ' + str(L_sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45152ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold corss validation of fully connected neural network\n",
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "from patsy import dmatrices\n",
    "kf=KFold(n_splits=10)\n",
    "y,X=dmatrices('DEATH_EVENT ~ age + anaemia + creatinine_phosphokinase  + ejection_fraction + high_blood_pressure + platelets + serum_creatinine + serum_sodium  + time',df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "for train,test in kf.split(x):\n",
    "    X_train,X_test=X[train],X[test]\n",
    "    y_train,y_test=y[train],y[test]\n",
    "    mlp=MLPClassifier()\n",
    "    mlp=mlp.fit(X_train,y_train)\n",
    "    scores.append(mlp.score(X_test,y_test))\n",
    "print(scores)\n",
    "sums=0\n",
    "for i in scores:\n",
    "    sums=i+sums\n",
    "D_sc=sums/10\n",
    "print('Scores of MLP is:'+str(D_sc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
